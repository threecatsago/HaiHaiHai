{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOB0BTtIxFl48DUqBciL5wA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/threecatsago/HaiHaiHai/blob/main/Gemini_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "环境准备：安装依赖库和设置API KEY"
      ],
      "metadata": {
        "id": "fQ1pK4fVln8g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c4InA6VW9g95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "适用于 Gemini API 的 Python SDK 包含在 google-generativeai 软件包。使用 pip 安装：\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S1hL7ScGtx7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "pmFQUZiut12C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "如需使用 Gemini API，您需要 API 密钥。如果您还没有账号， 在 Google AI Studio 中创建密钥。"
      ],
      "metadata": {
        "id": "al3WqEncuh4x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini 1.5 Flash 和 Gemini 1.5 Pro 免费层级的功能及价格\n",
        "\n",
        "| 功能 | Gemini 1.5 Flash | Gemini 1.5 Pro |\n",
        "|---|---|---|\n",
        "| 模型 | Gemini 1.5 Flash | Gemini 1.5 Pro |\n",
        "| 特点 | 最快的多模态模型，处理重复性任务出色，100万个上下文窗口 | 新一代模型，突破性的 200 万上下文窗口 |\n",
        "| 发布状态 | 正式推出，可用于生产环境 | 正式推出，可用于生产环境 |\n",
        "| **免费层级** |  |  |\n",
        "| API 服务 | 通过 API 服务提供，速率限制较低，用于测试目的 | 通过 API 服务提供，速率限制较低，用于测试目的 |\n",
        "| Google AI Studio | 在所有适用的国家/地区完全免费 | 在所有适用的国家/地区完全免费 |\n",
        "| **速率限制** |  |  |\n",
        "| 每分钟请求数 (RPM) | 15 | 2 |\n",
        "| 每分钟令牌数 (TPM) | 1,000,000 | 32,000 |\n",
        "| 每日请求数 (RPD) | 1,500 | 50 |\n",
        "| **价格** |  |  |\n",
        "| 输入价格 | 免费 | 免费 |\n",
        "| 输出价格 | 免费 | 免费 |\n",
        "| **上下文缓存** |  |  |\n",
        "| 存储 | 免费，每小时最多 100 万个令牌存储 | 不适用 |\n",
        "\n",
        "**总结:**\n",
        "\n",
        "* Gemini 1.5 Flash 更适合需要高吞吐量和较大上下文窗口（100 万）的任务，例如处理重复性任务。\n",
        "* Gemini 1.5 Pro 提供更大的上下文窗口（200 万），但免费层级的速率限制较低。它更适合需要处理更长文本或对话的场景，但目前不支持调整和上下文缓存。\n"
      ],
      "metadata": {
        "id": "9dymNbmQ-jrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "导入并配置。"
      ],
      "metadata": {
        "id": "FgzlDNzCuC8o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qiooAraIeRzh"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "# from google.colab import userdata\n",
        "\n",
        "# GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=\"AIzaSyCApd9R7-Qi6j4mjCnzZMkTKhSgyrzf398\")\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "\n",
        "# genai.configure(api_key=os.environ[\"API_KEY\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "发出第一个请求"
      ],
      "metadata": {
        "id": "TAESYTYUuMAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('models/gemini-1.5-flash')\n",
        "response = model.generate_content(\"响指是如何打响的\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "3k0XR6YHeVI-",
        "outputId": "8c4b0e9e-f3d7-4e03-f589-ac18230802ee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "响指的原理其实很简单，它利用了物理学中的「弹性」和「摩擦力」：\n",
            "\n",
            "1. **弹性**: 当你将拇指和中指的指尖并拢，并快速向外弹开时，你的指尖会产生一个快速压缩和膨胀的运动，这利用了手指的弹性。\n",
            "2. **摩擦力**: 当指尖快速弹开时，它们会与空气发生摩擦，产生一个快速的气流。这个气流会迅速移动，导致周围的空气压力急剧变化，从而产生一个短暂的、高频率的声波，这就是我们听到的响指声。\n",
            "\n",
            "响指的音调和响度取决于几个因素，包括：\n",
            "\n",
            "* **指尖的弹性**: 指尖越硬，弹性越大，响指的声音就越响亮。\n",
            "* **手指的运动速度**: 指尖弹开的速度越快，摩擦产生的气流越强，响指的声音就越响亮。\n",
            "* **手指的距离**: 指尖之间的距离越大，摩擦产生的气流就越强，响指的声音就越响亮。\n",
            "\n",
            "总的来说，响指是利用了简单的物理原理，将手指的弹性和摩擦力转化成了声音能量。 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemini API 支持文本，图像，Vision，音频，长上下文，代码执行，JSON 模式，函数调用，系统指令等生成功能。我们先从文本生成开始讲起，"
      ],
      "metadata": {
        "id": "pLle7pZlvitC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "# genai.configure(api_key=os.environ[\"API_KEY\"])"
      ],
      "metadata": {
        "id": "DZ_rQRDXANim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "通过API Key认证的genai就和我们的账户挂钩了，genai就相当于我们代码版的 ai studio，能关联到我们的账户，比如我们的文件，微调的模型等。在这里我们，可以理解它为和我们自己滴血结成契约的独有小精灵。"
      ],
      "metadata": {
        "id": "GYia_lUlATNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genai.list_files()\n",
        "\n",
        "import pprint\n",
        "for model in genai.list_files():\n",
        "    pprint.pprint(model)"
      ],
      "metadata": {
        "id": "Sk5uwxEJAow5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.list_models()\n",
        "\n",
        "import pprint\n",
        "for model in genai.list_models():\n",
        "    pprint.pprint(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "collapsed": true,
        "id": "qePF6PK549B8",
        "outputId": "496ea0d4-e701-4058-cda1-5c0328426042"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(name='models/chat-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 Chat (Legacy)',\n",
            "      description='A legacy text-only model optimized for chat conversations',\n",
            "      input_token_limit=4096,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
            "      temperature=0.25,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/text-bison-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='PaLM 2 (Legacy)',\n",
            "      description='A legacy model that understands text and generates text as an output',\n",
            "      input_token_limit=8196,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
            "      temperature=0.7,\n",
            "      max_temperature=None,\n",
            "      top_p=0.95,\n",
            "      top_k=40)\n",
            "Model(name='models/embedding-gecko-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding Gecko',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=1024,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Latest',\n",
            "      description=('The best model for scaling across a wide range of tasks. This is the latest '\n",
            "                   'model.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro',\n",
            "      description='The best model for scaling across a wide range of tasks',\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro 001 (Tuning)',\n",
            "      description=('The best model for scaling across a wide range of tasks. This is a stable '\n",
            "                   'model that supports tuning.'),\n",
            "      input_token_limit=30720,\n",
            "      output_token_limit=2048,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=0.9,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=None)\n",
            "Model(name='models/gemini-1.0-pro-vision-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description='The best image understanding model to handle a broad range of applications',\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-pro-vision',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.0 Pro Vision',\n",
            "      description='The best image understanding model to handle a broad range of applications',\n",
            "      input_token_limit=12288,\n",
            "      output_token_limit=4096,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=0.4,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=32)\n",
            "Model(name='models/gemini-1.5-pro-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro Latest',\n",
            "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro 001',\n",
            "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Pro',\n",
            "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-exp-0801',\n",
            "      base_model_id='',\n",
            "      version='exp-0801',\n",
            "      display_name='Gemini 1.5 Pro Experimental 0801',\n",
            "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-pro-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='exp-0827',\n",
            "      display_name='Gemini 1.5 Pro Experimental 0827',\n",
            "      description='Mid-size multimodal model that supports up to 2 million tokens',\n",
            "      input_token_limit=2097152,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-latest',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash Latest',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createCachedContent'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-001-tuning',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 001 Tuning',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=16384,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens', 'createTunedModel'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='exp-0827',\n",
            "      display_name='Gemini 1.5 Flash Experimental 0827',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/gemini-1.5-flash-8b-exp-0827',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Gemini 1.5 Flash 8B Experimental 0827',\n",
            "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
            "      input_token_limit=1048576,\n",
            "      output_token_limit=8192,\n",
            "      supported_generation_methods=['generateContent', 'countTokens'],\n",
            "      temperature=1.0,\n",
            "      max_temperature=2.0,\n",
            "      top_p=0.95,\n",
            "      top_k=64)\n",
            "Model(name='models/embedding-001',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Embedding 001',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/text-embedding-004',\n",
            "      base_model_id='',\n",
            "      version='004',\n",
            "      display_name='Text Embedding 004',\n",
            "      description='Obtain a distributed representation of a text.',\n",
            "      input_token_limit=2048,\n",
            "      output_token_limit=1,\n",
            "      supported_generation_methods=['embedContent'],\n",
            "      temperature=None,\n",
            "      max_temperature=None,\n",
            "      top_p=None,\n",
            "      top_k=None)\n",
            "Model(name='models/aqa',\n",
            "      base_model_id='',\n",
            "      version='001',\n",
            "      display_name='Model that performs Attributed Question Answering.',\n",
            "      description=('Model trained to return answers to questions that are grounded in provided '\n",
            "                   'sources, along with estimating answerable probability.'),\n",
            "      input_token_limit=7168,\n",
            "      output_token_limit=1024,\n",
            "      supported_generation_methods=['generateAnswer'],\n",
            "      temperature=0.2,\n",
            "      max_temperature=None,\n",
            "      top_p=1.0,\n",
            "      top_k=40)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "有了独特的的契约助手（由我们API KEY认证的）后，如果我们想要向大模型发起问话，需要什么必须的信息告诉助手呢？\n",
        "\n",
        "1、 使用哪个模型 2、我们的问题是什么"
      ],
      "metadata": {
        "id": "M0-jAQRZBdDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('models/gemini-1.5-pro-exp-0827')"
      ],
      "metadata": {
        "id": "VvroOxpV4PpA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model."
      ],
      "metadata": {
        "id": "qtN95VgpCfi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用generate_content函数来生成单一的"
      ],
      "metadata": {
        "id": "Hi9ogU7lCr2q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JXp4TBEEChuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.generate_content(\"响指是如何打响的\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "id": "kgfGH4bPCN84",
        "outputId": "bc307a71-138e-46d4-a327-a6358e660d1d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=protos.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"content\": {\n",
            "            \"parts\": [\n",
            "              {\n",
            "                \"text\": \"\\u54cd\\u6307\\u7684\\u54cd\\u58f0\\u5e76\\u975e\\u6765\\u81ea\\u624b\\u6307\\u95f4\\u7684\\u6469\\u64e6\\uff0c\\u800c\\u662f\\u6765\\u81ea\\u4e00\\u7cfb\\u5217\\u590d\\u6742\\u800c\\u5feb\\u901f\\u7684\\u8fd0\\u52a8\\u548c\\u529b\\u7684\\u4f5c\\u7528\\uff1a\\n\\n1. **\\u62c7\\u6307\\u84c4\\u529b:**  \\u9996\\u5148\\uff0c\\u4f60\\u9700\\u8981\\u7528\\u62c7\\u6307\\u6307\\u8179\\u7528\\u529b\\u6309\\u538b\\u4e2d\\u6307\\u7684\\u6307\\u8179\\u6216\\u8005\\u6307\\u5c16\\u3002 \\n2. **\\u4e2d\\u6307\\u84c4\\u52bf:**  \\u4e2d\\u6307\\u4f1a\\u88ab\\u62c7\\u6307\\u538b\\u5f2f\\uff0c\\u79ef\\u84c4\\u5f39\\u6027\\u52bf\\u80fd\\u3002\\n3. **\\u77ac\\u95f4\\u91ca\\u653e:**  \\u7136\\u540e\\uff0c\\u5feb\\u901f\\u5730\\u91ca\\u653e\\u4e2d\\u6307\\uff0c\\u8ba9\\u5b83\\u5411\\u4e0b\\u731b\\u70c8\\u5730\\u62cd\\u51fb\\u624b\\u638c\\u6839\\u90e8\\u9760\\u8fd1\\u5927\\u62c7\\u6307\\u6839\\u90e8\\u7684\\u80a5\\u539a\\u90e8\\u4f4d\\u3002 \\n4. **\\u51b2\\u51fb\\u548c\\u653e\\u5927:** \\u4e2d\\u6307\\u51fb\\u6253\\u624b\\u638c\\u4ea7\\u751f\\u7684\\u632f\\u52a8\\u4f1a\\u4f20\\u9012\\u5230\\u624b\\u638c\\u4ee5\\u53ca\\u4e0b\\u65b9\\u5f62\\u6210\\u7684\\u5bc6\\u95ed\\u7a7a\\u95f4\\u3002\\n5. **\\u7a7a\\u6c14\\u538b\\u7f29:**  \\u8fd9\\u4e2a\\u5bc6\\u95ed\\u7a7a\\u95f4\\u5185\\u7684\\u7a7a\\u6c14\\u88ab\\u8fc5\\u901f\\u538b\\u7f29\\u3002\\n6. **\\u54cd\\u58f0\\u4ea7\\u751f:**  \\u538b\\u7f29\\u7684\\u7a7a\\u6c14\\u51b2\\u7834\\u62c7\\u6307\\u548c\\u624b\\u638c\\u5f62\\u6210\\u7684\\u72ed\\u7a84\\u7f1d\\u9699\\uff0c\\u53d1\\u51fa\\u54cd\\u4eae\\u7684\\u201c\\u556a\\u201d\\u58f0\\u3002\\n\\n**\\u4e00\\u4e9b\\u5173\\u952e\\u70b9:**\\n\\n* **\\u901f\\u5ea6\\u662f\\u5173\\u952e:**  \\u4e2d\\u6307\\u91ca\\u653e\\u7684\\u901f\\u5ea6\\u8d8a\\u5feb\\uff0c\\u4ea7\\u751f\\u7684\\u54cd\\u58f0\\u5c31\\u8d8a\\u54cd\\u4eae\\u3002\\n* **\\u529b\\u91cf\\u4e5f\\u5f88\\u91cd\\u8981:** \\u62c7\\u6307\\u6309\\u538b\\u4e2d\\u6307\\u7684\\u529b\\u5ea6\\u8d8a\\u5927\\uff0c\\u4e2d\\u6307\\u79ef\\u84c4\\u7684\\u52bf\\u80fd\\u5c31\\u8d8a\\u591a\\uff0c\\u62cd\\u51fb\\u7684\\u529b\\u91cf\\u4e5f\\u5c31\\u8d8a\\u5927\\u3002\\n* **\\u5bc6\\u95ed\\u7a7a\\u95f4:** \\u624b\\u638c\\u4e0e\\u4e0b\\u65b9\\u5f62\\u6210\\u7684\\u5bc6\\u95ed\\u7a7a\\u95f4\\u662f\\u58f0\\u97f3\\u653e\\u5927\\u7684\\u5173\\u952e\\u3002\\n* **\\u624b\\u638c\\u7684\\u5f62\\u72b6:**  \\u624b\\u638c\\u7684\\u5f62\\u72b6\\u548c\\u5927\\u5c0f\\u4e5f\\u4f1a\\u5f71\\u54cd\\u54cd\\u6307\\u7684\\u54cd\\u5ea6\\u548c\\u97f3\\u8c03\\u3002\\n\\n**\\u5e76\\u975e\\u6bcf\\u4e2a\\u4eba\\u90fd\\u80fd\\u8f7b\\u677e\\u5730\\u6253\\u51fa\\u54cd\\u4eae\\u7684\\u54cd\\u6307\\uff0c\\u8fd9\\u9700\\u8981\\u4e00\\u5b9a\\u7684\\u7ec3\\u4e60\\u548c\\u6280\\u5de7\\u3002**  \\u6709\\u4e9b\\u4eba\\u5929\\u751f\\u624b\\u638c\\u7684\\u7ed3\\u6784\\u5c31\\u66f4\\u9002\\u5408\\u6253\\u54cd\\u6307\\uff0c\\u800c\\u6709\\u4e9b\\u4eba\\u5219\\u9700\\u8981\\u66f4\\u591a\\u7684\\u7ec3\\u4e60\\u624d\\u80fd\\u638c\\u63e1\\u8fd9\\u4e2a\\u6280\\u5de7\\u3002 \\n\\n\\u5e0c\\u671b\\u8fd9\\u4e2a\\u89e3\\u91ca\\u80fd\\u591f\\u5e2e\\u52a9\\u4f60\\u7406\\u89e3\\u54cd\\u6307\\u662f\\u5982\\u4f55\\u6253\\u54cd\\u7684! \\n\"\n",
            "              }\n",
            "            ],\n",
            "            \"role\": \"model\"\n",
            "          },\n",
            "          \"finish_reason\": \"STOP\",\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            },\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            },\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            },\n",
            "            {\n",
            "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
            "              \"probability\": \"NEGLIGIBLE\"\n",
            "            }\n",
            "          ]\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 7,\n",
            "        \"candidates_token_count\": 360,\n",
            "        \"total_token_count\": 367\n",
            "      }\n",
            "    }),\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "chat = model.start_chat(\n",
        "    history=[\n",
        "        {\"role\": \"user\", \"parts\": \"Hello\"},\n",
        "        {\"role\": \"model\", \"parts\": \"Great to meet you. What would you like to know?\"},\n",
        "    ]\n",
        ")\n",
        "response = chat.send_message(\"I have 2 dogs in my house.\")\n",
        "print(response.text)\n",
        "response = chat.send_message(\"How many paws are in my house?\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "JefrAL-N11tK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}